import React, { useState, useRef, useEffect } from 'react';
import './VoiceRecorder.css';
import api from '../services/api';

const VoiceRecorder = ({ onRecordComplete, maxDuration = 10 }) => {
  const [isRecording, setIsRecording] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [audioBlob, setAudioBlob] = useState(null);
  const [transcribedText, setTranscribedText] = useState('');
  const [isTranscribing, setIsTranscribing] = useState(false);

  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const timerRef = useRef(null);

  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, []);

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // ä½¿ç”¨ MediaRecorder å½•åˆ¶éŸ³é¢‘
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        setAudioBlob(audioBlob);
        stream.getTracks().forEach(track => track.stop());

        // å½•éŸ³ç»“æŸåè‡ªåŠ¨è°ƒç”¨åç«¯è¯­éŸ³è¯†åˆ«
        transcribeAudio(audioBlob);
      };

      mediaRecorder.start();
      setIsRecording(true);
      setRecordingTime(0);
      setTranscribedText('');

      // å¯åŠ¨è®¡æ—¶å™¨ï¼ˆä¸é™åˆ¶æ—¶é•¿ï¼‰
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);

    } catch (error) {
      console.error('æ— æ³•è®¿é—®éº¦å…‹é£:', error);
      alert('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);

      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    }
  };

  // è°ƒç”¨åç«¯ API è¿›è¡Œè¯­éŸ³è¯†åˆ«
  const transcribeAudio = async (audioBlob) => {
    try {
      setIsTranscribing(true);
      console.log('å¼€å§‹è¯­éŸ³è¯†åˆ«...');

      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      const response = await api.post('/danmaku/transcribe', formData, {
        headers: {
          'Content-Type': 'multipart/form-data'
        }
      });

      if (response.data.text) {
        setTranscribedText(response.data.text);
        console.log('è¯­éŸ³è¯†åˆ«æˆåŠŸ:', response.data.text);
      }
    } catch (error) {
      console.error('è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
      // è¯†åˆ«å¤±è´¥ä¸å½±å“å½•éŸ³åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ç›´æ¥å‘é€
    } finally {
      setIsTranscribing(false);
    }
  };

  const handleConfirm = () => {
    if (!audioBlob) {
      alert('è¯·å…ˆå½•åˆ¶è¯­éŸ³');
      return;
    }

    // å¦‚æœæœ‰è¯†åˆ«æ–‡æœ¬å°±ç”¨è¯†åˆ«æ–‡æœ¬ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æ–‡æœ¬
    const finalText = transcribedText || 'è¯­éŸ³å¼¹å¹•';

    onRecordComplete({
      audioBlob,
      text: finalText.trim(),
      duration: recordingTime
    });
    resetRecorder();
  };

  const handleDirectSend = () => {
    if (!audioBlob) {
      alert('è¯·å…ˆå½•åˆ¶è¯­éŸ³');
      return;
    }

    onRecordComplete({
      audioBlob,
      text: 'è¯­éŸ³å¼¹å¹•',
      duration: recordingTime
    });
    resetRecorder();
  };

  const resetRecorder = () => {
    setAudioBlob(null);
    setTranscribedText('');
    setRecordingTime(0);
  };

  return (
    <div className="voice-recorder">
      <div className="recorder-controls">
        {!isRecording && !audioBlob && (
          <button className="record-btn" onClick={startRecording}>
            ğŸ¤ å¼€å§‹å½•éŸ³
          </button>
        )}

        {isRecording && (
          <div className="recording-status">
            <button className="stop-btn" onClick={stopRecording}>
              â¹ åœæ­¢å½•éŸ³
            </button>
            <span className="recording-time">
              {recordingTime}s
            </span>
            <span className="recording-indicator">â— å½•éŸ³ä¸­...</span>
          </div>
        )}

        {audioBlob && (
          <div className="recorded-audio">
            <audio controls src={URL.createObjectURL(audioBlob)} />
            <div className="transcribed-text">
              <strong>è¯†åˆ«æ–‡æœ¬ï¼š</strong>
              {isTranscribing ? 'æ­£åœ¨è¯†åˆ«...' : transcribedText || 'æœªè¯†åˆ«åˆ°æ–‡æœ¬'}
            </div>

            {!transcribedText && !isTranscribing && (
              <div className="no-text-options">
                <p className="options-hint">æœªè¯†åˆ«åˆ°æ–‡æœ¬ï¼Œè¯·é€‰æ‹©ï¼š</p>
                <div className="option-buttons">
                  <button className="option-btn direct-send" onClick={handleDirectSend}>
                    ç›´æ¥å‘é€ï¼ˆæ˜¾ç¤º"è¯­éŸ³å¼¹å¹•"ï¼‰
                  </button>
                  <button className="option-btn manual-input" onClick={resetRecorder}>
                    é‡æ–°å½•åˆ¶
                  </button>
                </div>
              </div>
            )}

            <div className="action-buttons">
              {transcribedText && (
                <>
                  <button className="confirm-btn" onClick={handleConfirm}>
                    âœ“ ç¡®è®¤å‘é€
                  </button>
                  <button className="option-btn manual-input" onClick={resetRecorder}>
                    é‡æ–°å½•åˆ¶
                  </button>
                  <button className="option-btn clear-btn" onClick={resetRecorder}>
                    æ¸…é™¤
                  </button>
                </>
              )}
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

export default VoiceRecorder;
